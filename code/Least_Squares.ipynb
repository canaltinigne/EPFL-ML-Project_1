{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cross_validation import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 15)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"pure_features.npy\")\n",
    "y_train = np.load(\"reg_y.npy\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[1]):\n",
    "    X_train[np.where(X_train[:,i] == -999)[0], i] = np.median(X_train[np.where(X_train[:,i] != -999)[0], i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 105)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 7\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(2, degree+1):\n",
    "        new_col = X_train[:,i]**j\n",
    "        X_train = np.column_stack((X_train, new_col))\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# In the test section\n",
    "for i in range(X_train.shape[1]):\n",
    "    col_val = X_train[:, i]\n",
    "    X_train[:, i] = (col_val - np.min(col_val)) / (np.max(col_val) - np.min(col_val)) \n",
    "    \n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Rank Deficiency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train rank: 6\n",
      "(250000, 97)\n"
     ]
    }
   ],
   "source": [
    "# Rank of train set must be equal to the feature number\n",
    "# in order not to have Rank deficiency veya ill condition.\n",
    "\n",
    "print(\"X_train rank: {}\".format(np.linalg.matrix_rank(X_train)))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89]\n"
     ]
    }
   ],
   "source": [
    "qr = np.linalg.qr(X_train)[1]\n",
    "arr = []\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    if np.sum(abs(qr[i])) < 0.3:\n",
    "        arr.append(i)\n",
    "        \n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[89, 98, 103, 104]\n",
    "[89, 100]\n",
    "89\n",
    "89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, np.s_[89, 98, 103, 104], axis=1)  \n",
    "X_train = np.delete(X_train, np.s_[89, 100] , axis=1)  \n",
    "X_train = np.delete(X_train, np.s_[89] , axis=1)  \n",
    "X_train = np.delete(X_train, np.s_[89] , axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4]\n"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares\n",
    "\n",
    "par = {}\n",
    "par['threshold'] = np.linspace(-0.4, 0.4, 9)\n",
    "print(par['threshold'])\n",
    "acc_tr, acc_te = cross_validation(y_train, X_train, 5, h_pars=par, model='least')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "[[-0.4       0.720349]\n",
      " [-0.3       0.749722]\n",
      " [-0.2       0.770933]\n",
      " [-0.1       0.784642]\n",
      " [ 0.        0.790046]\n",
      " [ 0.1       0.787257]\n",
      " [ 0.2       0.776894]\n",
      " [ 0.3       0.759253]\n",
      " [ 0.4       0.736502]]\n",
      "Valid Set:\n",
      "[[-0.4       0.71978 ]\n",
      " [-0.3       0.749092]\n",
      " [-0.2       0.770864]\n",
      " [-0.1       0.784512]\n",
      " [ 0.        0.79008 ]\n",
      " [ 0.1       0.787128]\n",
      " [ 0.2       0.776508]\n",
      " [ 0.3       0.758712]\n",
      " [ 0.4       0.736416]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set:\")\n",
    "print(acc_tr)\n",
    "print(\"Valid Set:\")\n",
    "print(acc_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_X shape: (568238, 30)\n",
      "\n",
      "Test_Y shape: (568238,)\n",
      "\n",
      "Column names: ['DER_mass_MMC' 'DER_mass_transverse_met_lep' 'DER_mass_vis' 'DER_pt_h'\n",
      " 'DER_deltaeta_jet_jet' 'DER_mass_jet_jet' 'DER_prodeta_jet_jet'\n",
      " 'DER_deltar_tau_lep' 'DER_pt_tot' 'DER_sum_pt' 'DER_pt_ratio_lep_tau'\n",
      " 'DER_met_phi_centrality' 'DER_lep_eta_centrality' 'PRI_tau_pt'\n",
      " 'PRI_tau_eta' 'PRI_tau_phi' 'PRI_lep_pt' 'PRI_lep_eta' 'PRI_lep_phi'\n",
      " 'PRI_met' 'PRI_met_phi' 'PRI_met_sumet' 'PRI_jet_num'\n",
      " 'PRI_jet_leading_pt' 'PRI_jet_leading_eta' 'PRI_jet_leading_phi'\n",
      " 'PRI_jet_subleading_pt' 'PRI_jet_subleading_eta' 'PRI_jet_subleading_phi'\n",
      " 'PRI_jet_all_pt']\n",
      "\n",
      "Column shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import load_csv_data\n",
    "\n",
    "test_set = load_csv_data('../test.csv')\n",
    "y_test, X_test, ids, columns = test_set\n",
    "\n",
    "print(\"Test_X shape: {}\".format(X_test.shape))\n",
    "print(\"\\nTest_Y shape: {}\".format(y_test.shape))\n",
    "print(\"\\nColumn names: {}\".format(columns))\n",
    "print(\"\\nColumn shape: {}\".format(columns.shape))\n",
    "\n",
    "n_features = len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 15)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = np.sort([0,4,5,6,12,23,3,9,11,1,10,13,21,22,29])\n",
    "X_test = X_test[:, selected_features]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 15)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_transformed_columns = [3,9,13,21,29,5,23]\n",
    "dummy = np.zeros(X_test.shape[0]).reshape(X_test.shape[0], -1)\n",
    "\n",
    "for x, i in enumerate(selected_features):\n",
    "    \n",
    "    added_column = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    if i in log_transformed_columns:\n",
    "        if i in [5,23]:\n",
    "            added_column[np.where(X_test[:,x] != -999)] = np.log(X_test[np.where(X_test[:,x] != -999),x] + 1)\n",
    "            added_column[np.where(X_test[:,x] == -999)] = -999\n",
    "        else:\n",
    "            added_column = np.log(X_test[:, x] + 1)\n",
    "    else:\n",
    "        added_column = X_test[:, x]\n",
    "        \n",
    "    dummy = np.column_stack((dummy, added_column))\n",
    "\n",
    "X_test = np.delete(dummy,0,1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 15)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(X_test.shape[1]):\n",
    "    X_test[np.where(X_test[:,i] == -999)[0], i] = np.median(X_test[np.where(X_test[:,i] != -999)[0], i])\n",
    "    \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 105)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 7\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    for j in range(2, degree+1):\n",
    "        new_col = X_test[:,i]**j\n",
    "        X_test = np.column_stack((X_test, new_col))\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 97)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.delete(X_test, np.s_[89, 98, 103, 104], axis=1)  \n",
    "X_test = np.delete(X_test, np.s_[89, 100] , axis=1)  \n",
    "X_test = np.delete(X_test, np.s_[89] , axis=1)  \n",
    "X_test = np.delete(X_test, np.s_[89] , axis=1)  \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 97)\n",
      "-0.27276547867787504\n",
      "31.267262426454877\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_test.shape[1]):\n",
    "    col_val = X_train[:, i]\n",
    "    X_test[:, i] = (X_test[:, i] - np.min(col_val)) / (np.max(col_val) - np.min(col_val)) \n",
    "    \n",
    "print(X_test.shape)\n",
    "print(X_test.min())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import predict_labels\n",
    "\n",
    "w, _ = least_squares(y_train, X_train)\n",
    "predictions = predict_labels(w, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(ids, predictions, 'output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versiyonlar:\n",
    "\n",
    "1) Threshold = 0, degree=3 Polynomial Feature, Normalization, Imputation w/ median, log transformation = 0.77418\n",
    "\n",
    "2) Threshold = 0, degree=6 Polynomial Feature, Normalization, Imputation w/ median, Bazı featurelar çıkarıldı rank deficiency önlemek için sırasıyla 89 77 87 77 77 columnlar log transformation = 0.78783\n",
    "\n",
    "3) Threshold = 0, degree=7 Polynomial Feature, Normalization, Imputation w/ test_set median, Bazı featurelar çıkarıldı rank deficiency önlemek için sırasıyla [89, 98, 103, 104] [89, 100] 89 89 columnlar log transformation = 0.79094"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
